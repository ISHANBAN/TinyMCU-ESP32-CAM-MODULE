# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VCsEkLwrxBrqQ-ova8-OVFAVjCeNkeCA
"""

# Commented out IPython magic to ensure Python compatibility.
# MCUNet Training for ESP32-CAM
# Complete notebook for training and preparing models for ESP32-CAM module

# ========= SECTION 1: SETUP AND INSTALLATION =========
# Install necessary packages and clone the repository

# Check if GPU is available (recommended for faster training)
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU Model: {torch.cuda.get_device_name(0)}")

# Clone the MCUNet repository
!git clone https://github.com/mit-han-lab/mcunet.git
# %cd mcunet

# Install common dependencies

# Uninstall existing TensorFlow and TensorFlow Addons to avoid conflicts
!pip uninstall -y tensorflow tensorflow-addons

# Install specific compatible versions of TensorFlow and TensorFlow Addons
# TensorFlow 2.10 and TensorFlow Addons 0.18.0 are known to be compatible
!pip install torch torchvision numpy matplotlib pillow tqdm tensorflow==2.10.0 tensorflow-addons==0.18.0
!pip install PyYAML easydict
!pip install onnx
!pip install onnx-tf # This was also needed for the conversion
# ========= SECTION 2: PREPARE YOUR DATASET =========
# You can either use a standard dataset like CIFAR-10 or your custom dataset

# Option 1: Use CIFAR-10 (simple classification dataset with 10 classes)
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader, random_split

# Define image transformations for training
transform_train = transforms.Compose([
    transforms.Resize((96, 96)),  # Resize to model input size
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Define image transformations for validation
transform_val = transforms.Compose([
    transforms.Resize((96, 96)),  # Same size as training
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load CIFAR-10 dataset
train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)
test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform_val)

# Split training data into training and validation sets
train_size = int(0.8 * len(train_dataset))
val_size = len(train_dataset) - train_size
train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])

print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
print(f"Test samples: {len(test_dataset)}")

# Create data loaders
batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

# Display some sample images
import matplotlib.pyplot as plt
import numpy as np
import torchvision
def imshow(img):
    img = img / 2 + 0.5  # Unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.axis('off')
    plt.show()

# Get some random training images
dataiter = iter(train_loader)
images, labels = next(dataiter)

# Show images
imshow(torchvision.utils.make_grid(images[:4]))
# Print labels
classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
print(' '.join('%5s' % classes[labels[j]] for j in range(4)))
# ========= SECTION 3: BUILD THE MODEL =========
# Define a small CNN model suitable for ESP32-CAM

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

class MCUNetModel(nn.Module):
    def __init__(self, num_classes=10):
        super(MCUNetModel, self).__init__()

        # This model is designed to be small enough for ESP32-CAM
        # Input: 96x96x3

        # First convolutional block
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(16)
        self.pool1 = nn.MaxPool2d(2, 2)  # Output: 48x48x16

        # Second convolutional block
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(32)
        self.pool2 = nn.MaxPool2d(2, 2)  # Output: 24x24x32

        # Third convolutional block
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.pool3 = nn.MaxPool2d(2, 2)  # Output: 12x12x64

        # Fourth convolutional block
        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn4 = nn.BatchNorm2d(128)
        self.pool4 = nn.MaxPool2d(2, 2)  # Output: 6x6x128

        # Classifier
        self.fc1 = nn.Linear(6 * 6 * 128, 256)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, num_classes)

    def forward(self, x):
        # Conv blocks
        x = self.pool1(F.relu(self.bn1(self.conv1(x))))
        x = self.pool2(F.relu(self.bn2(self.conv2(x))))
        x = self.pool3(F.relu(self.bn3(self.conv3(x))))
        x = self.pool4(F.relu(self.bn4(self.conv4(x))))

        # Flatten
        x = x.view(x.size(0), -1)

        # Fully connected
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)

        return x

# Initialize the model
model = MCUNetModel(num_classes=10)  # 10 classes for CIFAR-10
print(model)

# Calculate model size (parameters)
def count_parameters(model):
    return sum(p.numel() for p in model.parameters())

print(f"Total parameters: {count_parameters(model):,}")

# Estimate model size in MB (very rough estimation)
model_params_bytes = count_parameters(model) * 4  # float32 is 4 bytes
quantized_model_bytes = count_parameters(model)  # int8 is 1 byte
print(f"Estimated model size (float32): {model_params_bytes / (1024 * 1024):.2f} MB")
print(f"Estimated model size after quantization (int8): {quantized_model_bytes / (1024 * 1024):.2f} MB")

# ========= SECTION 4: TRAIN THE MODEL =========
# Train and validate the model

# Set up training parameters
num_epochs = 20
learning_rate = 0.001

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)

# Training and validation functions
def train(model, train_loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward + backward + optimize
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # Statistics
        running_loss += loss.item() * inputs.size(0)
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    train_loss = running_loss / total
    train_acc = correct / total
    return train_loss, train_acc

def validate(model, val_loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # Statistics
            running_loss += loss.item() * inputs.size(0)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    val_loss = running_loss / total
    val_acc = correct / total
    return val_loss, val_acc

# Set device (GPU if available, otherwise CPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
print(f"Training on {device}")

# Training loop
best_val_loss = float('inf')
best_val_acc = 0.0

# Lists to store metrics for plotting
train_losses = []
val_losses = []
train_accs = []
val_accs = []

for epoch in range(num_epochs):
    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)
    val_loss, val_acc = validate(model, val_loader, criterion, device)

    # Learning rate scheduling
    scheduler.step(val_loss)

    # Save metrics for plotting
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_accs.append(train_acc)
    val_accs.append(val_acc)

    # Print progress
    print(f"Epoch {epoch+1}/{num_epochs}: "
          f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, "
          f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

    # Save best model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        best_val_loss = val_loss
        torch.save(model.state_dict(), 'best_model.pth')
        print(f"Best model saved with validation accuracy: {best_val_acc:.4f}")

# Load best model
model.load_state_dict(torch.load('best_model.pth'))

# Plot training progress
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Training and Validation Loss')

plt.subplot(1, 2, 2)
plt.plot(train_accs, label='Train Acc')
plt.plot(val_accs, label='Val Acc')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')

plt.tight_layout()
plt.show()

# Evaluate on test set
test_loss, test_acc = validate(model, test_loader, criterion, device)
print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}")

# ========= SECTION 5: CONVERT AND OPTIMIZE MODEL FOR ESP32-CAM =========
# Convert PyTorch model to TFLite and quantize it
!pip install tensorflow==2.10.0 tensorflow-addons==0.18.0 onnx-tf
try:
    import tensorflow as tf
    print(f"Successfully imported TensorFlow version: {tf.__version__}")
except ModuleNotFoundError:
    print("TensorFlow module not found after installation attempt. Please restart your kernel or environment.")
    # You might want to add sys.exit(1) here if running as a script
    raise # Re-raise the exception to stop execution

import numpy as np
import os
!pip install tensorflow-addons
# Function to convert PyTorch model to TFLite format
def convert_pytorch_to_tflite(model, input_shape=(1, 3, 96, 96), output_path='model.tflite'):
    # Set model to evaluation mode
    model.eval()

    # Create a dummy input
    dummy_input = torch.randn(input_shape).to(device)

    # ONNX export
    torch.onnx.export(model, dummy_input, 'model.onnx',
                     input_names=['input'], output_names=['output'],
                     dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}})

    print("ONNX model exported to model.onnx")

    # Convert ONNX model to TensorFlow (this requires onnx-tf package)
    !pip install onnx-tf
    import onnx
    from onnx_tf.backend import prepare

    onnx_model = onnx.load('model.onnx')
    tf_rep = prepare(onnx_model)
    tf_rep.export_graph('tf_model')

    print("TensorFlow model created at tf_model/")

    # Load the TensorFlow model
    converter = tf.lite.TFLiteConverter.from_saved_model('tf_model')
    tflite_model = converter.convert()

    # Save the TFLite model
    with open(output_path, 'wb') as f:
        f.write(tflite_model)

    print(f"TFLite model saved to {output_path}")
    return tflite_model

# Function to create a representative dataset for quantization
def representative_dataset_gen():
    # Use some validation samples for calibration
    for i, (inputs, _) in enumerate(val_loader):
        if i > 10:  # Just need a small subset for calibration
            break
        input_numpy = inputs.numpy()
        for single_input in input_numpy:
            # Need to transpose from PyTorch format (C,H,W) to TensorFlow format (H,W,C)
            single_input = np.transpose(single_input, (1, 2, 0))
            yield [np.array(single_input, dtype=np.float32).reshape(1, 96, 96, 3)]

# Convert to TFLite
tflite_model = convert_pytorch_to_tflite(model)

# Now quantize the TFLite model to int8 for better performance on ESP32
# This significantly reduces model size and improves inference speed
converter = tf.lite.TFLiteConverter.from_saved_model('tf_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
quantized_tflite_model = converter.convert()

# Save the quantized model
with open('model_quantized.tflite', 'wb') as f:
    f.write(quantized_tflite_model)

print(f"Quantized model saved to model_quantized.tflite")
print(f"Original model size: {len(tflite_model) / 1024:.2f} KB")
print(f"Quantized model size: {len(quantized_tflite_model) / 1024:.2f} KB")

# ========= SECTION 6: PREPARE MODEL FOR ESP32-CAM =========
# Create C code for ESP32-CAM

# Since TinyEngine's code generation tools might not be directly accessible,
# we'll create a basic C code template that you can use with ESP32-CAM

def create_c_model_header(model_path, output_path, num_classes=10):
    """
    Create a C header file with the model data for ESP32-CAM
    """
    # Read the quantized model data
    with open(model_path, 'rb') as f:
        model_data = f.read()

    # Create the header file
    with open(f"{output_path}/model.h", 'w') as f:
        f.write("#ifndef MODEL_H\n")
        f.write("#define MODEL_H\n\n")

        f.write("#include <stdint.h>\n\n")

        # Model parameters
        f.write("#define MODEL_INPUT_WIDTH 96\n")
        f.write("#define MODEL_INPUT_HEIGHT 96\n")
        f.write("#define MODEL_INPUT_CHANNELS 3\n")
        f.write(f"#define MODEL_NUM_CLASSES {num_classes}\n\n")

        # Class names (for CIFAR-10)
        f.write("const char* class_names[] = {\n")
        for name in classes:
            f.write(f'    "{name}",\n')
        f.write("};\n\n")

        # Model data as a C array
        f.write(f"const unsigned char model_data[{len(model_data)}] = {{\n    ")
        bytes_per_line = 12
        for i, byte in enumerate(model_data):
            f.write(f"0x{byte:02x}")
            if i < len(model_data) - 1:
                f.write(", ")
                if (i + 1) % bytes_per_line == 0:
                    f.write("\n    ")
        f.write("\n};\n\n")

        f.write("#endif // MODEL_H\n")

# Create output directory
os.makedirs('esp32cam_model', exist_ok=True)

# Create model header
create_c_model_header('model_quantized.tflite', 'esp32cam_model', num_classes=10)

# Create helper files for ESP32-CAM
with open('esp32cam_model/esp32cam_inference.cpp', 'w') as f:
    f.write("""
#include "esp32cam_inference.h"
#include "model.h"
#include "tensorflow/lite/micro/all_ops_resolver.h"
#include "tensorflow/lite/micro/micro_error_reporter.h"
#include "tensorflow/lite/micro/micro_interpreter.h"
#include "tensorflow/lite/schema/schema_generated.h"

// Globals, used for compatibility with Arduino-style sketches
namespace {
    tflite::ErrorReporter* error_reporter = nullptr;
    const tflite::Model* model = nullptr;
    tflite::MicroInterpreter* interpreter = nullptr;
    TfLiteTensor* input = nullptr;
    TfLiteTensor* output = nullptr;

    // Create an area of memory to use for input, output, and intermediate arrays
    // The size of this will depend on the model you're using, and may need to be
    // determined by experimentation.
    constexpr int kTensorArenaSize = 120 * 1024;
    static uint8_t tensor_arena[kTensorArenaSize];
}  // namespace

bool setup_model() {
    // Set up logging
    static tflite::MicroErrorReporter micro_error_reporter;
    error_reporter = &micro_error_reporter;

    // Map the model into a usable data structure
    model = tflite::GetModel(model_data);
    if (model->version() != TFLITE_SCHEMA_VERSION) {
        TF_LITE_REPORT_ERROR(error_reporter,
                            "Model schema version %d not equal "
                            "to supported version %d.",
                            model->version(), TFLITE_SCHEMA_VERSION);
        return false;
    }

    // Pull in all the operation implementations we need
    static tflite::AllOpsResolver resolver;

    // Build an interpreter to run the model
    static tflite::MicroInterpreter static_interpreter(
        model, resolver, tensor_arena, kTensorArenaSize, error_reporter);
    interpreter = &static_interpreter;

    // Allocate memory from the tensor_arena for the model's tensors
    TfLiteStatus allocate_status = interpreter->AllocateTensors();
    if (allocate_status != kTfLiteOk) {
        TF_LITE_REPORT_ERROR(error_reporter, "AllocateTensors() failed");
        return false;
    }

    // Get information about the model's input and output tensors
    input = interpreter->input(0);
    output = interpreter->output(0);

    return true;
}

bool perform_inference(uint8_t* image_data, int8_t* results) {
    // Ensure the model is set up
    if (!interpreter) {
        return false;
    }

    // Copy image data to model input tensor
    for (int i = 0; i < MODEL_INPUT_WIDTH * MODEL_INPUT_HEIGHT * MODEL_INPUT_CHANNELS; i++) {
        // Convert to int8_t with zero point at 128
        input->data.int8[i] = static_cast<int8_t>(image_data[i] - 128);
    }

    // Run inference
    TfLiteStatus invoke_status = interpreter->Invoke();
    if (invoke_status != kTfLiteOk) {
        return false;
    }

    // Copy output results
    for (int i = 0; i < MODEL_NUM_CLASSES; i++) {
        results[i] = output->data.int8[i];
    }

    return true;
}

int get_top_result(int8_t* results) {
    int top_index = 0;
    int8_t top_score = results[0];

    for (int i = 1; i < MODEL_NUM_CLASSES; i++) {
        if (results[i] > top_score) {
            top_score = results[i];
            top_index = i;
        }
    }

    return top_index;
}
""")

with open('esp32cam_model/esp32cam_inference.h', 'w') as f:
    f.write("""
#ifndef ESP32CAM_INFERENCE_H
#define ESP32CAM_INFERENCE_H

#include <stdint.h>

// Initialize the TF Lite model
bool setup_model();

// Run inference on the provided image data
// image_data should be a flat array of 96x96x3 values in RGB format
// results will contain the output scores for each class
bool perform_inference(uint8_t* image_data, int8_t* results);

// Get the index of the class with the highest score
int get_top_result(int8_t* results);

#endif // ESP32CAM_INFERENCE_H
""")
# Create a README.md file with instructions
with open('esp32cam_model/README.md', 'w') as f:
    f.write("""# ESP32-CAM Model""")